import cv2
import numpy as np
import os
import logging
import random
from typing import List, Tuple, Optional, Dict
from PIL import Image, ImageDraw, ImageFont

from config import (
    APP_CONFIG,
    SUBTITLE_CONFIG,
    Characters,
    Backgrounds,
    Expressions,
    Paths,
)

logger = logging.getLogger(__name__)


class VideoProcessor:
    def __init__(self):
        self.fps = APP_CONFIG.fps
        self.resolution = APP_CONFIG.resolution
        self.characters = Characters.get_all()
        self.subtitle_config = SUBTITLE_CONFIG

        # ItemManagerã‚’importã—ã¦åˆæœŸåŒ–ï¼ˆé…å»¶importï¼‰
        from src.services.item_manager import ItemManager

        self.item_manager = ItemManager()

        # ãƒ•ã‚©ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._cached_font = None

        # ç¬ãè¨­å®š
        self.blink_config = {
            "min_interval": 2.0,  # æœ€å°ç¬ãé–“éš”ï¼ˆç§’ï¼‰
            "max_interval": 6.0,  # æœ€å¤§ç¬ãé–“éš”ï¼ˆç§’ï¼‰
            "duration": 0.15,  # ç¬ãã®é•·ã•ï¼ˆç§’ï¼‰
        }

    def load_character_images(
        self, character_name: str = "zundamon", expression: str = "normal"
    ) -> Dict[str, np.ndarray]:
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”»åƒã‚’èª­ã¿è¾¼ã¿ï¼ˆç‰¹å®šã®è¡¨æƒ…ï¼‰"""
        base_path = Paths.get_character_dir(character_name)
        expression_dir = os.path.join(base_path, expression)

        images = {}
        image_files = {
            "closed": f"{expression}_closed.png",
            "half": f"{expression}_half.png",
            "open": f"{expression}_open.png",
            "blink": f"{expression}_blink.png",  # ç¬ãç”¨ç”»åƒ
        }

        for key, filename in image_files.items():
            path = os.path.join(expression_dir, filename)

            if os.path.exists(path):
                img = cv2.imread(path, cv2.IMREAD_UNCHANGED)
                if img is not None:
                    images[key] = img
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šnormalè¡¨æƒ…ã‚’è©¦ã™
                if expression != "normal":
                    fallback_path = os.path.join(
                        base_path, "normal", f"normal_{key}.png"
                    )
                    if os.path.exists(fallback_path):
                        img = cv2.imread(fallback_path, cv2.IMREAD_UNCHANGED)
                        if img is not None:
                            images[key] = img
                            continue

        if images:
            # å…¨ç”»åƒã‚’åŒã˜ã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚º
            target_size = images[list(images.keys())[0]].shape[:2]
            for key in images:
                images[key] = cv2.resize(images[key], (target_size[1], target_size[0]))

        return images

    def get_available_expressions(self, character_name: str) -> List[str]:
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§åˆ©ç”¨å¯èƒ½ãªè¡¨æƒ…ã‚’å–å¾—"""
        base_path = Paths.get_character_dir(character_name)
        if not os.path.exists(base_path):
            return []

        expressions = set()

        # ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‹ã‚‰è¡¨æƒ…ã‚’å–å¾—
        for item in os.listdir(base_path):
            item_path = os.path.join(base_path, item)
            if os.path.isdir(item_path):
                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                required_files = [
                    f"{item}_closed.png",
                    f"{item}_half.png",
                    f"{item}_open.png",
                ]
                if any(
                    os.path.exists(os.path.join(item_path, file))
                    for file in required_files
                ):
                    expressions.add(item)

        # config.pyã§å®šç¾©ã•ã‚ŒãŸè¡¨æƒ…ã®ã¿ã‚’è¿”ã™
        available_expressions = Expressions.get_available_names()
        return [expr for expr in available_expressions if expr in expressions]

    def load_character_images_all_expressions(
        self, character_name: str
    ) -> Dict[str, Dict[str, np.ndarray]]:
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å…¨è¡¨æƒ…ç”»åƒã‚’èª­ã¿è¾¼ã¿"""
        available_expressions = self.get_available_expressions(character_name)
        if not available_expressions:
            available_expressions = ["normal"]

        all_expressions = {}
        for expression in available_expressions:
            expr_images = self.load_character_images(character_name, expression)
            if expr_images:
                all_expressions[expression] = expr_images

        return all_expressions

    def load_all_character_images(self) -> Dict[str, Dict[str, Dict[str, np.ndarray]]]:
        """å…¨ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å…¨è¡¨æƒ…ç”»åƒã‚’èª­ã¿è¾¼ã¿"""
        all_images = {}
        for char_key, char_info in self.characters.items():
            char_expressions = self.load_character_images_all_expressions(char_key)
            if char_expressions:
                all_images[char_key] = char_expressions

        return all_images

    def load_backgrounds(self) -> Dict[str, np.ndarray]:
        """ã™ã¹ã¦ã®èƒŒæ™¯ç”»åƒã‚’èª­ã¿è¾¼ã¿"""
        bg_dir = Paths.get_backgrounds_dir()

        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èƒŒæ™¯è¨­å®šã‚’èª­ã¿è¾¼ã¿
        json_path = os.path.join(bg_dir, "backgrounds.json")
        if os.path.exists(json_path):
            Backgrounds.load_backgrounds_from_json(json_path)
            logger.info(f"Loaded background configurations from {json_path}")

        backgrounds = {}

        if not os.path.exists(bg_dir):
            logger.error(f"Background directory not found: {bg_dir}")
            return backgrounds

        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹ç”»åƒæ‹¡å¼µå­
        supported_extensions = Backgrounds.get_supported_extensions()

        for filename in os.listdir(bg_dir):
            file_path = os.path.join(bg_dir, filename)

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
            if not os.path.isfile(file_path):
                continue

            # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
            _, ext = os.path.splitext(filename.lower())
            if ext not in supported_extensions:
                continue

            try:
                # ç”»åƒèª­ã¿è¾¼ã¿
                bg = cv2.imread(file_path)
                if bg is not None:
                    # ãƒªã‚µã‚¤ã‚º
                    bg = cv2.resize(bg, self.resolution)

                    bg_name = os.path.splitext(filename)[0]
                    backgrounds[bg_name] = bg

                    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‹•çš„ã«èƒŒæ™¯è¨­å®šã‚’ç™»éŒ²
                    if not Backgrounds.is_valid_background(bg_name):
                        Backgrounds.register_background(
                            name=bg_name,
                            display_name=bg_name.replace("_", " ").title(),
                            emoji="ğŸ–¼ï¸",
                            description=f"èƒŒæ™¯ç”»åƒ: {bg_name}"
                        )

            except Exception as e:
                logger.error(f"Error loading background {file_path}: {e}")

        if "default_bg" in backgrounds:
            backgrounds["default"] = backgrounds["default_bg"]
        elif backgrounds:
            first_key = list(backgrounds.keys())[0]
            backgrounds["default"] = backgrounds[first_key]

        if not backgrounds:
            logger.error("No background images found")

        return backgrounds

    def get_background_names(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªèƒŒæ™¯ç”»åƒåã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        backgrounds = self.load_backgrounds()
        names = [name for name in backgrounds.keys() if name != "default"]
        return sorted(names)

    def generate_blink_timings(
        self, total_duration: float, character_name: str = None
    ) -> List[Dict]:
        """ç¬ãã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’ç”Ÿæˆ"""
        blink_times = []
        current_time = random.uniform(1.0, 3.0)

        while current_time < total_duration - self.blink_config["duration"]:
            # ç¬ãã®é–‹å§‹ã¨çµ‚äº†æ™‚é–“
            blink_start = current_time
            blink_end = current_time + self.blink_config["duration"]

            blink_times.append(
                {"start": blink_start, "end": blink_end, "character": character_name}
            )

            # æ¬¡ã®ç¬ãã¾ã§ã®é–“éš”ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«æ±ºå®š
            interval = random.uniform(
                self.blink_config["min_interval"], self.blink_config["max_interval"]
            )
            current_time += interval

        return blink_times

    def is_character_blinking(
        self, current_time: float, blink_timings: List[Dict], character_name: str
    ) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒç¾åœ¨ç¬ãã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        for blink in blink_timings:
            if (
                blink.get("character") == character_name
                or blink.get("character") is None
            ):
                if blink["start"] <= current_time <= blink["end"]:
                    return True
        return False

    def select_mouth_image(
        self, intensity: float, images: dict, is_blinking: bool = False
    ) -> np.ndarray:
        """éŸ³å£°å¼·åº¦ã«åŸºã¥ãå£ç”»åƒé¸æŠï¼ˆç¬ãå¯¾å¿œï¼‰"""
        # ç¬ãä¸­ã¯ç›®ã‚’é–‰ã˜ãŸç”»åƒã‚’å„ªå…ˆï¼ˆç¬ãå°‚ç”¨ç”»åƒãŒãªã‘ã‚Œã°closedã‚’ä½¿ç”¨ï¼‰
        if is_blinking:
            return images.get(
                "blink", images.get("closed", images[list(images.keys())[0]])
            )

        # é€šå¸¸ã®å£ãƒ‘ã‚¯ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
        if intensity < 0.1:
            return images.get("closed", images[list(images.keys())[0]])
        elif intensity < 0.4:
            return images.get(
                "half", images.get("closed", images[list(images.keys())[0]])
            )
        else:
            return images.get(
                "open", images.get("half", images[list(images.keys())[0]])
            )

    def composite_frame(
        self,
        background: np.ndarray,
        character: np.ndarray,
        position: Tuple[int, int] = None,
    ) -> np.ndarray:
        """ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’èƒŒæ™¯ã«åˆæˆ"""
        if position is None:
            bg_h, bg_w = background.shape[:2]
            char_h, char_w = character.shape[:2]
            position = ((bg_w - char_w) // 2, (bg_h - char_h) // 2)

        x, y = position
        result = background.copy()

        if character.shape[2] == 4:
            char_rgb = character[:, :, :3]
            char_alpha = character[:, :, 3] / 255.0

            char_h, char_w = character.shape[:2]
            bg_h, bg_w = background.shape[:2]

            # å¢ƒç•Œãƒã‚§ãƒƒã‚¯ - ä½ç½®ãŒè² ã®å€¤ã«ãªã‚‹å ´åˆã‚‚è€ƒæ…®
            if y < 0:
                char_rgb = char_rgb[-y:, :]
                char_alpha = char_alpha[-y:, :]
                char_h += y
                y = 0

            if x < 0:
                char_rgb = char_rgb[:, -x:]
                char_alpha = char_alpha[:, -x:]
                char_w += x
                x = 0

            if y + char_h > bg_h:
                char_h = bg_h - y
                if char_h > 0:
                    char_rgb = char_rgb[:char_h, :]
                    char_alpha = char_alpha[:char_h, :]

            if x + char_w > bg_w:
                char_w = bg_w - x
                if char_w > 0:
                    char_rgb = char_rgb[:, :char_w]
                    char_alpha = char_alpha[:, :char_w]

            # æœ‰åŠ¹ãªé ˜åŸŸãŒã‚ã‚‹å ´åˆã®ã¿åˆæˆ
            if (
                char_h > 0
                and char_w > 0
                and char_rgb.shape[0] > 0
                and char_rgb.shape[1] > 0
            ):
                # ã‚¢ãƒ«ãƒ•ã‚¡ãƒãƒ£ãƒ³ãƒãƒ«ã®æ¬¡å…ƒã‚’æ‹¡å¼µ
                char_alpha_expanded = char_alpha[:, :, np.newaxis]
                for c in range(3):
                    result[y : y + char_h, x : x + char_w, c] = (
                        char_alpha_expanded[:, :, 0] * char_rgb[:, :, c]
                        + (1 - char_alpha_expanded[:, :, 0])
                        * background[y : y + char_h, x : x + char_w, c]
                    )
        else:
            char_h, char_w = character.shape[:2]
            result[y : y + char_h, x : x + char_w] = character

        return result

    def composite_conversation_frame(
        self,
        background: np.ndarray,
        character_images: Dict[str, Dict[str, Dict[str, np.ndarray]]],
        active_speakers: Dict[str, Dict[str, any]],
        conversation_mode: str = "duo",
        current_time: float = 0.0,
        blink_timings: List[Dict] = None,
        character_items: Dict[str, str] = None,
    ) -> np.ndarray:
        """ä¼šè©±ç”¨ã®ãƒ•ãƒ¬ãƒ¼ãƒ åˆæˆï¼ˆè¡¨æƒ…å¯¾å¿œï¼‰"""
        result = background.copy()

        if conversation_mode == "solo":
            # ã‚½ãƒ­ãƒ¢ãƒ¼ãƒ‰ï¼šè©±ã—ã¦ã„ã‚‹ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ã¿è¡¨ç¤º
            sorted_chars = []
            for char_name, speaker_data in active_speakers.items():
                intensity = 0
                if isinstance(speaker_data, dict):
                    intensity = speaker_data.get("intensity", 0)
                else:
                    intensity = float(speaker_data) if speaker_data else 0

                if intensity > 0.1:  # å®Ÿéš›ã«è©±ã—ã¦ã„ã‚‹å ´åˆã®ã¿
                    sorted_chars.append((char_name, speaker_data))
        else:
            sorted_chars = sorted(
                active_speakers.items(),
                key=lambda x: self.characters.get(
                    x[0], Characters.ZUNDAMON
                ).x_offset_ratio,
            )

        for char_name, speaker_data in sorted_chars:
            if char_name not in character_images or char_name not in self.characters:
                continue

            # è¡¨æƒ…ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆå¾Œæ–¹äº’æ›æ€§ã‚’è€ƒæ…®ï¼‰
            if isinstance(speaker_data, dict):
                intensity = speaker_data.get("intensity", 0.0)
                expression = speaker_data.get("expression", "normal")
            else:
                # æ—§å½¢å¼ï¼ˆfloatï¼‰ã®å ´åˆ
                intensity = float(speaker_data)
                expression = "normal"

            # è¡¨æƒ…ç”»åƒã‚»ãƒƒãƒˆã®å–å¾—
            if expression in character_images[char_name]:
                char_imgs = character_images[char_name][expression]
            elif "normal" in character_images[char_name]:
                char_imgs = character_images[char_name]["normal"]
            else:
                # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸè¡¨æƒ…ã‚’ä½¿ç”¨
                available_expressions = list(character_images[char_name].keys())
                if available_expressions:
                    char_imgs = character_images[char_name][available_expressions[0]]
                else:
                    logger.error(f"No expressions available for {char_name}")
                    continue

            # ç¬ããƒã‚§ãƒƒã‚¯
            is_blinking = False
            if blink_timings:
                is_blinking = self.is_character_blinking(
                    current_time, blink_timings, char_name
                )

            # å£ç”»åƒé¸æŠï¼ˆç¬ãã‚’è€ƒæ…®ï¼‰
            mouth_img = self.select_mouth_image(intensity, char_imgs, is_blinking)

            bg_h, bg_w = background.shape[:2]
            char_h, char_w = mouth_img.shape[:2]

            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã‹ã‚‰ã‚µã‚¤ã‚ºæ¯”ç‡ã‚’å–å¾—
            char_config = self.characters.get(char_name, Characters.ZUNDAMON)
            unified_size_ratio = char_config.size_ratio
            target_height = int(bg_h * unified_size_ratio)
            target_width = int(char_w * target_height / char_h)

            # ä½ç½®è¨ˆç®—ï¼šå¸¸ã«config.pyã®è¨­å®šã‚’ä½¿ç”¨
            x_offset_ratio = char_config.x_offset_ratio
            x = int(bg_w * x_offset_ratio - target_width // 2)

            y = int(bg_h * char_config.y_offset_ratio)

            # ã‚µã‚¤ã‚ºåˆ¶é™
            if target_width > bg_w * 0.8:
                target_width = int(bg_w * 0.8)
                target_height = int(char_h * target_width / char_w)

            mouth_img = cv2.resize(mouth_img, (target_width, target_height))

            margin = 10
            x = max(-target_width // 3, min(x, bg_w - target_width // 3 * 2))
            y = max(margin, min(y, bg_h - target_height - margin))

            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆæˆ
            result = self.composite_frame(result, mouth_img, (x, y))

            # ã‚¢ã‚¤ãƒ†ãƒ åˆæˆ
            if character_items and char_name in character_items:
                item_name = character_items[char_name]
                if item_name and item_name != "none":
                    try:
                        result = self.item_manager.composite_item_on_frame(
                            result,
                            item_name,
                            char_name,
                            (x, y),
                            (target_width, target_height),
                        )
                    except Exception as e:
                        logger.error(
                            f"Failed to composite item {item_name} for {char_name}: {e}"
                        )

        return result

    def get_japanese_font(self) -> Optional[ImageFont.FreeTypeFont]:
        """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
        if self._cached_font is not None:
            return self._cached_font

        local_fonts_dir = Paths.get_fonts_dir()

        # å„ªå…ˆé †ä½ä»˜ããƒ•ã‚©ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        priority_fonts = [
            "NotoSansJP-Black.ttf",
        ]

        local_font_paths = []

        if os.path.exists(local_fonts_dir):
            for priority_font in priority_fonts:
                priority_path = os.path.join(local_fonts_dir, priority_font)
                if os.path.exists(priority_path):
                    local_font_paths.append(priority_path)

        font_paths = local_font_paths

        for font_path in font_paths:
            if os.path.exists(font_path):
                try:
                    if font_path.lower().endswith(".ttc"):
                        for index in range(15):
                            try:
                                font = ImageFont.truetype(
                                    font_path,
                                    self.subtitle_config.font_size,
                                    index=index,
                                )
                            except (OSError, IOError, ValueError):
                                continue
                    else:
                        # TTF/OTFãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
                        font = ImageFont.truetype(
                            font_path, self.subtitle_config.font_size
                        )
                        # æ—¥æœ¬èªæ–‡å­—ã§ãƒ†ã‚¹ãƒˆ
                        try:
                            self._cached_font = font
                            logger.info(f"Successfully loaded font: {font_path}")
                            return font
                        except Exception:
                            continue
                except Exception as e:
                    logger.warning(f"Failed to load font {font_path}: {e}")
                    continue

        logger.warning("No suitable Japanese font found, using default font")
        try:
            default_font = ImageFont.load_default()
            try:
                self._cached_font = default_font
                return default_font
            except Exception:
                logger.warning("Default font does not support Japanese characters")
                self._cached_font = default_font
                return default_font
        except Exception as e:
            logger.error(f"Failed to load default font: {e}")
            return None

    def draw_subtitle_on_frame(
        self,
        frame: np.ndarray,
        text: str,
        progress: float = 1.0,
        speaker: str = "zundamon",
    ) -> np.ndarray:
        """å­—å¹•ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ã«æç”»"""
        if not text.strip():
            return frame

        try:
            # PILå½¢å¼ã«å¤‰æ›
            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(pil_image)

            # ãƒ•ã‚©ãƒ³ãƒˆå–å¾—
            font = self.get_japanese_font()
            if font is None:
                logger.warning("No font available")
                return frame

            # ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºè¨ˆç®—
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]

            # èƒŒæ™¯ã‚µã‚¤ã‚ºè¨ˆç®—
            padding_x = self.subtitle_config.padding_x
            padding_top = self.subtitle_config.padding_top
            padding_bottom = self.subtitle_config.padding_bottom
            border_width = self.subtitle_config.border_width
            bg_width = text_width + (padding_x * 2)
            bg_height = text_height + padding_top + padding_bottom

            # ä½ç½®è¨ˆç®—
            bg_x = (self.resolution[0] - bg_width) // 2
            bg_y = self.resolution[1] - self.subtitle_config.margin_bottom - bg_height

            text_x = bg_x + padding_x
            text_y = bg_y + padding_top

            # RGBAå¤‰æ›
            if pil_image.mode != "RGBA":
                pil_image = pil_image.convert("RGBA")
                draw = ImageDraw.Draw(pil_image)

            # è©±è€…ã«å¿œã˜ãŸè‰²è¨­å®š
            radius = self.subtitle_config.border_radius
            bg_color = self.subtitle_config.background_color
            border_color = self.characters.get(
                speaker, Characters.ZUNDAMON
            ).subtitle_color

            # èƒŒæ™¯æç”»
            try:
                draw.rounded_rectangle(
                    [
                        bg_x - border_width,
                        bg_y - border_width,
                        bg_x + bg_width + border_width,
                        bg_y + bg_height + border_width,
                    ],
                    radius + border_width,
                    fill=border_color,
                )
                draw.rounded_rectangle(
                    [bg_x, bg_y, bg_x + bg_width, bg_y + bg_height],
                    radius,
                    fill=bg_color,
                )
            except AttributeError:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                draw.rectangle(
                    [
                        bg_x - border_width,
                        bg_y - border_width,
                        bg_x + bg_width + border_width,
                        bg_y + bg_height + border_width,
                    ],
                    fill=border_color,
                )
                draw.rectangle(
                    [bg_x, bg_y, bg_x + bg_width, bg_y + bg_height], fill=bg_color
                )

            # ãƒ†ã‚­ã‚¹ãƒˆæç”»
            outline_width = self.subtitle_config.outline_width
            outline_color = self.subtitle_config.outline_color
            text_color = self.subtitle_config.font_color

            # ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³
            for dx in range(-outline_width, outline_width + 1):
                for dy in range(-outline_width, outline_width + 1):
                    if dx != 0 or dy != 0:
                        draw.text(
                            (text_x + dx, text_y + dy),
                            text,
                            font=font,
                            fill=outline_color,
                        )

            # ãƒ¡ã‚¤ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            draw.text((text_x, text_y), text, font=font, fill=text_color)

            # BGRå½¢å¼ã«æˆ»ã™
            return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

        except Exception as e:
            logger.error(f"Subtitle drawing failed: {e}")
            return frame
